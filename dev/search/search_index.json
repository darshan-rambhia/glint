{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Glint","text":"<p>Lightweight Proxmox monitoring dashboard. Single binary, ~10MB, ~30-50MB RAM.</p> <p>Go + templ + htmx --- server-rendered HTML with 15-second live polling. SQLite for history. No JavaScript build step.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Node monitoring --- CPU, memory, swap, root filesystem, load average, I/O wait, uptime</li> <li>Guest monitoring --- LXC containers and QEMU VMs with status, CPU, memory, disk, network</li> <li>PBS backup tracking --- datastore usage, backup snapshots, task history, stale backup detection</li> <li>S.M.A.R.T. disk health --- ATA and NVMe attribute parsing with Backblaze-derived failure rate thresholds</li> <li>Alerting --- ntfy and webhook notifications with configurable rules and deduplication</li> <li>Multi-node ready --- supports multiple PVE instances, clusters, and PBS servers</li> <li>Temperature monitoring --- optional SSH-based CPU temperature polling</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>You need a Proxmox VE API token before starting. If you don't have one yet, follow the Getting Started guide first --- it takes about 5 minutes.</p> <p>1. Create a config file called <code>glint.yml</code>:</p> <pre><code>listen: \":3800\"\ndb_path: \"/data/glint.db\"\n\npve:\n  - name: \"main\"\n    host: \"https://YOUR_PROXMOX_IP:8006\"        # (1)!\n    token_id: \"glint@pam!monitor\"                # (2)!\n    token_secret: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxx\" # (3)!\n    insecure: true                               # (4)!\n</code></pre> <ol> <li>Replace with your Proxmox server's IP address</li> <li>The API token ID you created (format: <code>user@realm!tokenname</code>)</li> <li>The token secret shown when you created the token</li> <li>Set to <code>true</code> if your Proxmox uses a self-signed certificate (most do)</li> </ol> <p>2. Start Glint:</p> Docker ComposeDocker RunHomebrewGo InstallBinary Download <p>Create a <code>docker-compose.yml</code> in the same folder as <code>glint.yml</code>:</p> <pre><code>services:\n  glint:\n    image: ghcr.io/darshan-rambhia/glint:latest\n    container_name: glint\n    restart: unless-stopped\n    ports:\n      - \"3800:3800\"\n    volumes:\n      - glint-data:/data\n      - ./glint.yml:/etc/glint/glint.yml:ro\n    command: [\"glint\", \"--config\", \"/etc/glint/glint.yml\"]\n\nvolumes:\n  glint-data:\n</code></pre> <pre><code>docker compose up -d\n</code></pre> <pre><code>docker run -d -p 3800:3800 \\\n  -v glint-data:/data \\\n  -v ./glint.yml:/etc/glint/glint.yml:ro \\\n  ghcr.io/darshan-rambhia/glint:latest \\\n  glint --config /etc/glint/glint.yml\n</code></pre> <pre><code>brew install darshan-rambhia/tap/glint\nglint --config glint.yml\n</code></pre> <p>Requires Go 1.26+ and a C compiler (for SQLite):</p> <pre><code>CGO_ENABLED=1 go install github.com/darshan-rambhia/glint/cmd/glint@latest\nglint --config glint.yml\n</code></pre> <p>Download the latest release from GitHub Releases, then:</p> <pre><code>glint --config glint.yml\n</code></pre> <p>See the Deployment guide for full binary install and systemd service setup.</p> <p>3. Open <code>http://localhost:3800</code> --- you should see your Proxmox node within 15 seconds.</p> <p>Want PBS backup monitoring too?</p> <p>Add a <code>pbs:</code> section to your config. See the full Configuration reference for all options including PBS, alerting, and notifications.</p>"},{"location":"#documentation","title":"Documentation","text":"Guide Description Getting Started Create API tokens, write your config, deploy with Docker Compose Configuration Full config reference --- YAML options, environment variables, defaults Deployment Docker, Podman, Quadlet, systemd bare metal Logging Log formats, levels, systemd/journald integration Architecture How Glint works --- collectors, cache, store, alerter Testing Unit tests, benchmarks, fuzz tests, coverage, linting"},{"location":"#inspiration","title":"Inspiration","text":"<p>Glint draws heavily from two excellent projects:</p> <ul> <li>Pulse --- Instance + node two-level hierarchy, per-instance collectors with a bounded worker pool, and snapshot-based caching. Pulse covered Proxmox host metrics and PBS backup monitoring well but lacked S.M.A.R.T. disk health tracking.</li> <li>Scrutiny --- WWN-based disk identity (globally unique, survives reboots and cable changes), protocol-aware SMART parsing (ATA vs NVMe vs SCSI), and Backblaze-derived failure rate thresholds for real-world risk assessment beyond manufacturer pass/fail.</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"api/","title":"API Reference","text":"<p>Glint exposes a JSON API alongside its htmx fragment endpoints. The full OpenAPI 2.0 specification is available at <code>/swagger/</code> when the server is running.</p> <p></p>"},{"location":"api/#endpoints-overview","title":"Endpoints Overview","text":""},{"location":"api/#json-api","title":"JSON API","text":"Method Path Description <code>GET</code> <code>/healthz</code> Health check with collector status <code>GET</code> <code>/api/sparkline/node/{instance}/{node}</code> Node metric sparkline data points <code>GET</code> <code>/api/sparkline/guest/{instance}/{vmid}</code> Guest CPU sparkline data points"},{"location":"api/#html-fragments-htmx","title":"HTML Fragments (htmx)","text":"Method Path Description <code>GET</code> <code>/</code> Full dashboard page <code>GET</code> <code>/fragments/nodes</code> Node status cards <code>GET</code> <code>/fragments/guests</code> Guest table <code>GET</code> <code>/fragments/backups</code> Backup status <code>GET</code> <code>/fragments/disks</code> Disk health table <code>GET</code> <code>/fragments/disk/{wwn}</code> Disk SMART detail <code>GET</code> <code>/fragments/sparkline/node/{instance}/{node}</code> Node sparkline SVG <code>GET</code> <code>/fragments/sparkline/guest/{instance}/{vmid}</code> Guest sparkline SVG"},{"location":"api/#swagger-ui","title":"Swagger UI","text":"<p>When running locally, Swagger UI is available at <code>http://localhost:3800/swagger/</code>.</p>"},{"location":"api/#regenerating-the-spec","title":"Regenerating the Spec","text":"<pre><code>task swagger\n</code></pre> <p>This runs <code>swag init</code> and outputs the OpenAPI spec to <code>docs/swagger/</code>.</p>"},{"location":"architecture/","title":"Architecture","text":"<p>Glint is a single Go binary that polls Proxmox APIs, caches results in memory, persists history to SQLite, and serves a server-rendered dashboard via htmx.</p>"},{"location":"architecture/#system-overview","title":"System Overview","text":"<pre><code>                      Browser (htmx polling every 15s)\n                              |\n                        HTTPS (reverse proxy)\n                              |\n                  +-----------+-----------+\n                  |         Glint         |\n                  |   Go binary (Docker)  |\n                  |                       |\n                  |  templ HTML handlers  |\n                  |         |             |\n                  |  Collector Engine     |\n                  |  (worker pool)        |\n                  |         |             |\n                  |  In-Memory Cache      |\n                  |  (sync.RWMutex)       |\n                  |         |             |\n                  |  SQLite               |\n                  |  (metadata + history) |\n                  +-----------+-----------+\n                   /          |          \\\n        PVE Instance 1   PVE Instance 2   PBS Instance 1\n        \u251c\u2500 Node A        \u251c\u2500 Node C        \u251c\u2500 Datastore X\n        \u2502  \u251c\u2500 CTs/VMs    \u2502  \u251c\u2500 CTs/VMs    \u2514\u2500 Datastore Y\n        \u2502  \u2514\u2500 Disks      \u2502  \u2514\u2500 Disks\n        \u2514\u2500 Node B        \u2514\u2500 Node D\n                                              ntfy (optional)\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":"<ol> <li>Collector goroutines (one per PVE/PBS instance) submit API calls to a bounded worker pool</li> <li>Each collector updates the in-memory cache and appends snapshots to SQLite</li> <li>HTTP handlers read from cache snapshots (lock-free for consumers)</li> <li>htmx polls fragment endpoints every 15s, swapping HTML in-place</li> <li>Alerter goroutine evaluates cache state against rules, sends to ntfy with deduplication</li> </ol>"},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":""},{"location":"architecture/#from-pulse","title":"From Pulse","text":"<ul> <li>Instance + Node hierarchy --- each PVE config entry is an \"instance\" (standalone or cluster). Nodes are auto-discovered via <code>GET /nodes</code>. Guests are scoped by cluster name to prevent duplicates.</li> <li>Per-instance collectors with worker pool --- all API calls go through a bounded worker pool (<code>chan struct{}</code> semaphore, default 4 workers) to cap memory and concurrent HTTP connections.</li> <li>Snapshot-based cache --- the cache exposes deep-copy snapshots so HTTP handlers never hold locks. Uses <code>sync.RWMutex</code> (not <code>sync.Map</code>) for atomic cross-key consistency.</li> </ul>"},{"location":"architecture/#from-scrutiny","title":"From Scrutiny","text":"<ul> <li>WWN-based disk identity --- disks are identified by World Wide Name (globally unique, survives reboots and cable changes), not <code>/dev/sdX</code>.</li> <li>Backblaze-derived SMART thresholds --- individual attributes are evaluated against real-world failure rate data from Backblaze's public hard drive statistics.</li> <li>Protocol-aware SMART parsing --- ATA, NVMe, and SCSI each have different data models, parsed separately.</li> </ul>"},{"location":"architecture/#project-structure","title":"Project Structure","text":"<pre><code>cmd/glint/main.go              Entry point, wiring, signal handling\ninternal/\n  api/                         HTTP handlers + htmx fragments\n    handlers.go                Route registration + fragment handlers\n    middleware.go              Logging, recovery\n  collector/                   Data collection from Proxmox APIs\n    collector.go               Collector interface + runner loop\n    backup.go                  BackupCollector interface\n    errors.go                  RetryableError, behavior-based error types\n    pve.go                     PVE client (nodes, guests, disks, SMART)\n    pbs.go                     PBS client (datastores, snapshots, tasks)\n    temperature.go             Optional SSH-based temp polling\n  smart/                       S.M.A.R.T. health assessment\n    evaluate.go                Attribute status evaluation\n    thresholds.go              Backblaze failure rate lookup tables\n    ata.go                     ATA attribute parsing\n    nvme.go                    NVMe text field parsing\n  store/                       SQLite persistence\n    store.go                   Repository (insert, query, migrate)\n    migrations.go              Embedded schema\n    pruner.go                  Retention cleanup\n  cache/                       Thread-safe in-memory state\n    cache.go                   Multi-instance cache with snapshots\n  alerter/                     Alert rule engine\n    alerter.go                 Rule evaluation + deduplication\n    templates.go               Default message templates\n  notify/                      Notification providers\n    provider.go                Provider interface + Notification struct\n    ntfy.go                    ntfy provider\n    webhook.go                 Generic webhook provider\n  config/                      Configuration loading\n    config.go                  YAML + env var parsing\n  model/                       Shared domain types\n    model.go                   Node, Guest, Disk, Backup, etc.\ntemplates/                     templ HTML templates\n  layout.templ                 Base HTML shell\n  dashboard.templ              Main page\n  nodes.templ                  Node cards\n  guests.templ                 Guest table\n  backups.templ                PBS backup panel\n  disks.templ                  Disk health table\n  disk_detail.templ            Expanded SMART attributes\n  components/                  Reusable UI components\nstatic/                        CSS + htmx.min.js\n</code></pre>"},{"location":"architecture/#collector-engine","title":"Collector Engine","text":"<p>Each PVE/PBS instance gets its own collector goroutine. All collectors share a single worker pool that bounds concurrent API calls.</p>"},{"location":"architecture/#worker-pool","title":"Worker Pool","text":"<pre><code>type WorkerPool struct {\n    sem chan struct{}  // buffered channel as semaphore\n}\n</code></pre> <p>Default: 4 workers. Configurable via <code>worker_pool_size</code>. When polling 4 nodes in parallel, only 4 HTTP requests fly at once --- prevents unbounded goroutine spawning.</p>"},{"location":"architecture/#pve-poll-cycle","title":"PVE Poll Cycle","text":"<pre><code>1. GET /nodes \u2192 discover/update node list\n2. For each online node (fan out via worker pool):\n   a. GET /nodes/{node}/status \u2192 host metrics\n   b. GET /nodes/{node}/lxc \u2192 containers\n   c. GET /nodes/{node}/qemu \u2192 VMs\n   d. If disk poll due (&gt;1h since last):\n      - GET /nodes/{node}/disks/list \u2192 disk inventory\n      - For each disk: GET /nodes/{node}/disks/smart \u2192 SMART data\n3. Merge results, dedup guests by cluster_id\n4. Update cache + write to SQLite\n</code></pre>"},{"location":"architecture/#pbs-poll-cycle","title":"PBS Poll Cycle","text":"<pre><code>1. GET /status/datastore-usage \u2192 all datastores\n2. For each monitored datastore:\n   a. GET /admin/datastore/{store}/snapshots \u2192 backup snapshots\n3. GET /nodes/localhost/tasks \u2192 recent tasks\n4. Update cache + write to SQLite\n</code></pre>"},{"location":"architecture/#polling-intervals","title":"Polling Intervals","text":"Data Interval Notes Node + guest metrics 15s Fan out across nodes in parallel S.M.A.R.T. disk data 1h Slow operation (1-5s per disk) PBS backups + tasks 5m Node discovery 5m Within PVE collector SSH temperatures 60s Graceful fallback if unavailable"},{"location":"architecture/#in-memory-cache","title":"In-Memory Cache","text":"<p>The cache stores the latest state from all collectors. HTTP handlers read snapshots (deep copies) so they never hold locks during template rendering.</p> <pre><code>type Cache struct {\n    mu sync.RWMutex\n\n    Nodes      map[string]map[string]*Node        // [instance][node]\n    Guests     map[string]map[int]*Guest           // [cluster_id][vmid]\n    Disks      map[string]*Disk                    // [wwn]\n    Datastores map[string]map[string]*DatastoreStatus\n    Backups    map[string]map[string]*Backup\n    Tasks      map[string][]*PBSTask\n    LastPoll   map[string]time.Time\n}\n</code></pre> <p>Why <code>sync.RWMutex</code> over <code>sync.Map</code>: rendering a dashboard page requires consistent data across nodes, guests, and disks simultaneously. <code>sync.Map</code> provides no cross-key consistency guarantees.</p>"},{"location":"architecture/#sqlite-schema","title":"SQLite Schema","text":""},{"location":"architecture/#time-series-tables","title":"Time-Series Tables","text":"<p>All time-series tables use <code>WITHOUT ROWID</code> with <code>ts</code>-leading composite primary keys. This makes them clustered B-trees ordered by time --- ideal for range queries and pruning.</p> Table Retention Primary Key <code>node_snapshots</code> 48h <code>(ts, instance, node)</code> <code>guest_snapshots</code> 48h <code>(ts, instance, vmid)</code> <code>smart_snapshots</code> 30d <code>(ts, wwn)</code> <code>backup_snapshots</code> 7d <code>(ts, pbs_instance, backup_id, backup_time)</code> <code>datastore_snapshots</code> 7d <code>(ts, pbs_instance, store_name)</code> <code>alert_log</code> 30d <code>(id)</code> autoincrement"},{"location":"architecture/#pruner","title":"Pruner","text":"<p>An hourly goroutine deletes rows older than the retention period. The <code>ts</code>-leading PK means <code>DELETE FROM X WHERE ts &lt; ?</code> is a fast range scan on the clustered index.</p>"},{"location":"architecture/#smart-health-assessment","title":"S.M.A.R.T. Health Assessment","text":"<p>Disk health uses a multi-level evaluation inspired by Scrutiny:</p>"},{"location":"architecture/#status-bitfield","title":"Status Bitfield","text":"Bit Value Meaning 0 <code>StatusPassed</code> All checks passed 1 <code>StatusFailedSmart</code> Manufacturer SMART says <code>FAILING_NOW</code> 2 <code>StatusWarnScrutiny</code> Backblaze data suggests elevated risk 4 <code>StatusFailedScrutiny</code> Backblaze data suggests high failure probability 8 <code>StatusUnknown</code> Disk disappeared / unreachable 16 <code>StatusInternalError</code> Parse failure, API timeout"},{"location":"architecture/#evaluation-order","title":"Evaluation Order","text":"<ol> <li>Manufacturer SMART says <code>FAILING_NOW</code> \u2192 <code>StatusFailedSmart</code></li> <li><code>IN_THE_PAST</code> \u2192 <code>StatusWarnScrutiny</code></li> <li>Look up raw value in Backblaze thresholds:<ul> <li>Critical attribute (IDs 5, 10, 187, 188, 196, 197, 198) + failure rate &gt;= 10% \u2192 <code>StatusFailedScrutiny</code></li> <li>Non-critical + failure rate &gt;= 20% \u2192 <code>StatusFailedScrutiny</code></li> <li>Non-critical + failure rate &gt;= 10% \u2192 <code>StatusWarnScrutiny</code></li> </ul> </li> <li>Device status = bitwise OR of all attribute statuses</li> </ol>"},{"location":"architecture/#nvme-handling","title":"NVMe Handling","text":"<p>NVMe drives don't return ATA-style attributes. Glint parses the raw <code>smartctl</code> text output for NVMe-specific fields (<code>critical_warning</code>, <code>available_spare</code>, <code>percentage_used</code>, <code>media_errors</code>, etc.) and applies NVMe-specific thresholds.</p>"},{"location":"architecture/#http-routes","title":"HTTP Routes","text":"Route Type Refresh Description <code>GET /</code> Full page --- Dashboard shell <code>GET /fragments/nodes</code> htmx 15s All node cards with sparklines <code>GET /fragments/guests</code> htmx 15s Guest table (all instances) <code>GET /fragments/backups</code> htmx 60s PBS backup status + tasks <code>GET /fragments/disks</code> htmx 300s S.M.A.R.T. health (all nodes) <code>GET /fragments/disk/{wwn}</code> htmx on-click Expanded attributes for one disk <code>GET /api/sparkline/node/{instance}/{node}</code> JSON on-demand Node sparkline data <code>GET /api/sparkline/guest/{instance}/{vmid}</code> JSON on-demand Guest sparkline data <code>GET /healthz</code> JSON --- Health check"},{"location":"architecture/#alerting","title":"Alerting","text":"<p>The alerter goroutine evaluates the cache state against configured rules on each poll cycle. Alerts are deduplicated with per-rule cooldowns to prevent notification storms.</p>"},{"location":"architecture/#built-in-rules","title":"Built-in Rules","text":"Alert Default Condition Cooldown Node CPU high &gt; 90% for 5min 1h Node offline status != \"online\" 30min Guest down not running for 2min 30min Backup stale last backup &gt; 36h 6h Backup failed PBS task error 1h Disk SMART failed manufacturer failure 6h Datastore full &gt; 85% used 6h"},{"location":"architecture/#notification-providers","title":"Notification Providers","text":"<p>Providers implement a common interface:</p> <pre><code>type Provider interface {\n    Name() string\n    Send(ctx context.Context, n Notification) error\n}\n</code></pre> <p>Built-in providers: ntfy (with priority mapping and tags) and webhook (generic JSON POST to any URL).</p>"},{"location":"architecture/#lifecycle-management","title":"Lifecycle Management","text":"<p>All goroutines accept a <code>context.Context</code> and exit when cancelled. The main function uses <code>errgroup</code> to manage the lifecycle:</p> <pre><code>g, ctx := errgroup.WithContext(ctx)\ng.Go(func() error { return pveCollector.Run(ctx) })\ng.Go(func() error { return pbsCollector.Run(ctx) })\ng.Go(func() error { return pruner.Run(ctx) })\ng.Go(func() error { return server.Run(ctx) })\n</code></pre> <p>Graceful shutdown on <code>SIGINT</code>/<code>SIGTERM</code>: collectors stop polling, in-flight requests complete, SQLite is closed cleanly.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Glint supports two configuration methods: a YAML config file (recommended for multi-instance setups) and environment variables (convenient for single-instance deployments).</p>"},{"location":"configuration/#config-file","title":"Config File","text":"<p>Pass a config file with <code>--config</code>:</p> <pre><code>glint --config /etc/glint/glint.yml\n</code></pre>"},{"location":"configuration/#full-reference","title":"Full Reference","text":""},{"location":"configuration/#server-settings","title":"Server Settings","text":"<pre><code>listen: \":3800\"           # (1)!\ndb_path: \"/data/glint.db\" # (2)!\nlog_level: \"info\"         # (3)!\nlog_format: \"text\"        # (4)!\nhistory_hours: 48         # (5)!\nworker_pool_size: 4       # (6)!\n</code></pre> <ol> <li>Address and port to bind the HTTP server. Default: <code>:3800</code></li> <li>Path to the SQLite database file. Must be writable. Default: <code>glint.db</code></li> <li>Log verbosity: <code>debug</code>, <code>info</code>, <code>warn</code>, <code>error</code>. Default: <code>info</code></li> <li>Log output format: <code>text</code> (human-readable) or <code>json</code> (structured). Default: <code>text</code></li> <li>Hours of metric history to retain for sparkline charts. Default: <code>48</code></li> <li>Maximum concurrent API calls across all collectors. Default: <code>4</code></li> </ol>"},{"location":"configuration/#pve-instances","title":"PVE Instances","text":"<p>At least one PVE instance is required.</p> <pre><code>pve:\n  - name: \"main\"                    # (1)!\n    host: \"https://pve:8006\"        # (2)!\n    token_id: \"glint@pam!monitor\"   # (3)!\n    token_secret: \"xxx-xxx-xxx\"     # (4)!\n    insecure: true                  # (5)!\n    poll_interval: \"15s\"            # (6)!\n    disk_poll_interval: \"1h\"        # (7)!\n    ssh:                            # (8)!\n      host: \"192.168.1.215\"\n      user: \"root\"\n      key_path: \"/config/ssh/id_ed25519\"\n</code></pre> <ol> <li>Unique label for this instance. Used in UI and alerts.</li> <li>PVE API endpoint URL (include port).</li> <li>API token in <code>user@realm!tokenname</code> format.</li> <li>Token secret from <code>pveum user token add</code>.</li> <li>Skip TLS certificate verification. Set <code>true</code> for self-signed certs. Default: <code>false</code></li> <li>How often to poll node and guest metrics. Default: <code>15s</code></li> <li>How often to poll S.M.A.R.T. disk data (slow operation). Default: <code>1h</code></li> <li>Optional SSH connection for CPU temperature monitoring.</li> </ol>"},{"location":"configuration/#pbs-instances","title":"PBS Instances","text":"<p>PBS monitoring is optional.</p> <pre><code>pbs:\n  - name: \"main-pbs\"                 # (1)!\n    host: \"https://pbs:8007\"          # (2)!\n    token_id: \"glint@pbs!monitor\"     # (3)!\n    token_secret: \"xxx-xxx-xxx\"       # (4)!\n    insecure: true                    # (5)!\n    datastores: [\"homelab\"]           # (6)!\n    poll_interval: \"5m\"               # (7)!\n</code></pre> <ol> <li>Unique label for this PBS instance.</li> <li>PBS API endpoint URL (include port).</li> <li>API token in <code>user@realm!tokenname</code> format.</li> <li>Token secret from <code>proxmox-backup-manager user generate-token</code>.</li> <li>Skip TLS certificate verification. Default: <code>false</code></li> <li>Datastores to monitor. Empty list = monitor all discovered datastores.</li> <li>How often to poll backup data. Default: <code>5m</code></li> </ol>"},{"location":"configuration/#notifications","title":"Notifications","text":"<pre><code>notifications:\n  - type: ntfy                        # (1)!\n    url: \"http://ntfy:8080\"           # (2)!\n    topic: \"homelab-alerts\"           # (3)!\n\n  - type: webhook                     # (4)!\n    url: \"https://hooks.example.com/glint\"\n    method: \"POST\"                    # (5)!\n    headers:                          # (6)!\n      Authorization: \"Bearer xxx\"\n</code></pre> <ol> <li>Provider type: <code>ntfy</code> or <code>webhook</code>.</li> <li>ntfy server URL.</li> <li>ntfy topic name.</li> <li>Generic webhook --- POSTs the full alert as JSON.</li> <li>HTTP method: <code>POST</code> (default) or <code>PUT</code>.</li> <li>Custom headers for authentication.</li> </ol>"},{"location":"configuration/#alert-rules","title":"Alert Rules","text":"<p>All alert rules are optional. Defaults are applied if omitted.</p> <pre><code>alerts:\n  node_cpu_high:\n    threshold: 90           # Percent CPU usage\n    duration: \"5m\"          # Must sustain for this long\n    severity: \"warning\"\n\n  guest_down:\n    grace_period: \"2m\"      # Ignore brief restarts\n    severity: \"critical\"\n\n  backup_stale:\n    max_age: \"36h\"          # Alert if last backup older than this\n    severity: \"warning\"\n\n  disk_smart_failed:\n    severity: \"critical\"\n\n  datastore_full:\n    threshold: 85           # Percent datastore usage\n    severity: \"warning\"\n</code></pre> Rule Default Threshold Default Severity Description <code>node_cpu_high</code> 90% for 5m warning Sustained high CPU <code>guest_down</code> 2m grace critical Guest not running <code>backup_stale</code> 36h warning No recent backup <code>disk_smart_failed</code> --- critical Manufacturer SMART failure <code>datastore_full</code> 85% warning PBS datastore near capacity"},{"location":"configuration/#environment-variables","title":"Environment Variables","text":"<p>For a single PVE + PBS instance, you can skip the config file entirely:</p>"},{"location":"configuration/#pve","title":"PVE","text":"Variable Description Required <code>GLINT_PVE_URL</code> PVE API endpoint (e.g., <code>https://pve:8006</code>) Yes <code>GLINT_PVE_TOKEN_ID</code> API token ID (e.g., <code>glint@pam!monitor</code>) Yes <code>GLINT_PVE_TOKEN_SECRET</code> API token secret Yes <code>GLINT_PVE_INSECURE</code> Skip TLS verification (<code>true</code>/<code>false</code>) No"},{"location":"configuration/#pbs","title":"PBS","text":"Variable Description Required <code>GLINT_PBS_URL</code> PBS API endpoint (e.g., <code>https://pbs:8007</code>) No <code>GLINT_PBS_TOKEN_ID</code> API token ID With PBS URL <code>GLINT_PBS_TOKEN_SECRET</code> API token secret With PBS URL <code>GLINT_PBS_DATASTORE</code> Datastore name to monitor No <code>GLINT_PBS_INSECURE</code> Skip TLS verification No"},{"location":"configuration/#notifications_1","title":"Notifications","text":"Variable Description Required <code>GLINT_NTFY_URL</code> ntfy server URL No <code>GLINT_NTFY_TOPIC</code> ntfy topic name With ntfy URL"},{"location":"configuration/#server","title":"Server","text":"Variable Description Default <code>GLINT_LISTEN</code> Bind address <code>:3800</code> <code>GLINT_DB_PATH</code> SQLite database path <code>glint.db</code> <code>GLINT_LOG_LEVEL</code> Log level <code>info</code> <code>GLINT_LOG_FORMAT</code> Log format (<code>text</code> or <code>json</code>) <code>text</code> <p>Config file takes precedence</p> <p>When both a config file and environment variables are set, the config file values take precedence. Environment variables are only used to build a default single-instance config when no config file is provided.</p>"},{"location":"configuration/#cluster-setup","title":"Cluster Setup","text":""},{"location":"configuration/#multi-node-pve-cluster","title":"Multi-Node PVE Cluster","text":"<p>Point Glint at any one node in your cluster. It auto-discovers all nodes via the <code>/nodes</code> endpoint and deduplicates guests by cluster ID.</p> <pre><code>pve:\n  - name: \"prod-cluster\"\n    host: \"https://ANY_CLUSTER_NODE:8006\"\n    token_id: \"glint@pam!monitor\"\n    token_secret: \"xxx\"\n    insecure: true\n</code></pre>"},{"location":"configuration/#multiple-independent-pve-instances","title":"Multiple Independent PVE Instances","text":"<p>For separate (non-clustered) Proxmox hosts, add multiple entries. Each needs its own API token.</p> <pre><code>pve:\n  - name: \"homelab\"\n    host: \"https://192.168.1.215:8006\"\n    token_id: \"glint@pam!monitor\"\n    token_secret: \"xxx\"\n    insecure: true\n\n  - name: \"remote\"\n    host: \"https://10.0.0.50:8006\"\n    token_id: \"glint@pam!monitor\"\n    token_secret: \"yyy\"\n    insecure: true\n</code></pre>"},{"location":"configuration/#version-info","title":"Version Info","text":"<p>Glint embeds build metadata in the binary:</p> <pre><code>glint --version\n</code></pre> <pre><code>glint v0.1.0\n  commit:    abc1234def5678 (clean)\n  built:     2026-02-16T12:00:00Z\n  go:        go1.26\n  platform:  linux/amd64\n</code></pre> <p>At startup, the same info is logged:</p> <pre><code>level=INFO msg=\"starting glint\" version=v0.1.0 commit=abc1234def5678 built=2026-02-16T12:00:00Z dirty=clean go=go1.26 listen=:3800\n</code></pre>"},{"location":"configuration/#security-considerations","title":"Security Considerations","text":"<ul> <li>Tokens are read-only. PVEAuditor and Audit roles cannot modify anything.</li> <li>Use <code>insecure: true</code> only for self-signed certificates. If you have proper TLS certs, set it to <code>false</code>.</li> <li>Store tokens securely. Use Docker secrets, environment variables from a secrets manager, or file-based secrets for production deployments.</li> <li>Network isolation. Glint only needs access to the PVE/PBS API ports (8006/8007). It does not need SSH access unless you enable temperature monitoring.</li> <li>Revoke tokens if compromised: <code>pveum user token remove glint@pam monitor</code></li> </ul>"},{"location":"deployment/","title":"Deployment","text":"<p>Glint can be deployed using Docker, Podman, or as a systemd service directly on a Linux machine (no containers).</p>"},{"location":"deployment/#docker","title":"Docker","text":""},{"location":"deployment/#docker-compose-recommended","title":"Docker Compose (Recommended)","text":"<p>This is the easiest way to run Glint. You need two files in the same folder:</p> <ol> <li>A <code>docker-compose.yml</code> file (tells Docker how to run Glint)</li> <li>A <code>glint.yml</code> file (tells Glint how to connect to your Proxmox server)</li> </ol> <p>If you haven't created <code>glint.yml</code> yet, see the Getting Started guide first.</p> <p>Create a file called <code>docker-compose.yml</code> with this content:</p> <pre><code>services:\n  glint:\n    image: ghcr.io/darshan-rambhia/glint:latest\n    container_name: glint\n    restart: unless-stopped\n    ports:\n      - \"3800:3800\"\n    volumes:\n      - glint-data:/data\n      - ./glint.yml:/etc/glint/glint.yml:ro\n    command: [\"glint\", \"--config\", \"/etc/glint/glint.yml\"]\n    deploy:\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 32M\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\nvolumes:\n  glint-data:\n</code></pre> <p>Then start Glint:</p> <pre><code>docker compose up -d\n</code></pre> <p>Scratch base image</p> <p>The Glint image uses a <code>scratch</code> base (no shell, no utilities), so in-container <code>HEALTHCHECK</code> is not available. Use <code>curl http://localhost:3800/healthz</code> from the host or configure health checks in your reverse proxy.</p>"},{"location":"deployment/#docker-compose-with-environment-variables","title":"Docker Compose with Environment Variables","text":"<p>If you only have one Proxmox server and one PBS server, you can skip the config file entirely and pass settings as environment variables instead:</p> <pre><code>services:\n  glint:\n    image: ghcr.io/darshan-rambhia/glint:latest\n    container_name: glint\n    restart: unless-stopped\n    ports:\n      - \"3800:3800\"\n    volumes:\n      - glint-data:/data\n    environment:\n      GLINT_PVE_URL: \"https://192.168.1.215:8006\"\n      GLINT_PVE_TOKEN_ID: \"glint@pam!monitor\"\n      GLINT_PVE_TOKEN_SECRET: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n      GLINT_PBS_URL: \"https://10.100.1.102:8007\"\n      GLINT_PBS_TOKEN_ID: \"glint@pbs!monitor\"\n      GLINT_PBS_TOKEN_SECRET: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n      GLINT_PBS_DATASTORE: \"homelab\"\n      GLINT_NTFY_URL: \"http://10.100.1.104:8080\"\n      GLINT_NTFY_TOPIC: \"homelab-alerts\"\n      GLINT_LOG_FORMAT: \"json\"\n    deploy:\n      resources:\n        limits:\n          memory: 128M\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\nvolumes:\n  glint-data:\n</code></pre> <p>Replace the placeholder values with your actual Proxmox IP addresses and API tokens. See the Getting Started guide for how to create API tokens.</p>"},{"location":"deployment/#docker-run-without-compose","title":"Docker Run (Without Compose)","text":"<p>If you prefer a single command instead of a compose file:</p> <pre><code>docker run -d \\\n  --name glint \\\n  --restart unless-stopped \\\n  -p 3800:3800 \\\n  -v glint-data:/data \\\n  -v $(pwd)/glint.yml:/etc/glint/glint.yml:ro \\\n  ghcr.io/darshan-rambhia/glint:latest \\\n  glint --config /etc/glint/glint.yml\n</code></pre> <p>What does each flag mean?</p> <ul> <li><code>-d</code> --- run in the background (detached)</li> <li><code>--name glint</code> --- give the container a name so you can refer to it later</li> <li><code>--restart unless-stopped</code> --- automatically restart if it crashes or the server reboots</li> <li><code>-p 3800:3800</code> --- make port 3800 accessible from your browser</li> <li><code>-v glint-data:/data</code> --- store Glint's database in a named volume (persists across restarts)</li> <li><code>-v $(pwd)/glint.yml:...</code> --- mount your config file into the container (read-only)</li> </ul>"},{"location":"deployment/#updating-docker","title":"Updating Docker","text":"<p>To update Glint to the latest version:</p> <pre><code># Pull the newest image\ndocker compose pull\n\n# Restart with the new image (your data is preserved)\ndocker compose up -d\n</code></pre>"},{"location":"deployment/#viewing-docker-logs","title":"Viewing Docker Logs","text":"<pre><code># Follow logs in real-time (press Ctrl+C to stop)\ndocker compose logs -f glint\n\n# Show the last 50 lines\ndocker compose logs --tail=50 glint\n</code></pre>"},{"location":"deployment/#stopping-docker","title":"Stopping Docker","text":"<pre><code># Stop Glint (keeps your data)\ndocker compose down\n\n# Stop and delete all data (start fresh)\ndocker compose down -v\n</code></pre>"},{"location":"deployment/#podman","title":"Podman","text":"<p>Podman works almost identically to Docker. If you already have Podman installed, you can use the same compose files from the Docker section above.</p>"},{"location":"deployment/#podman-run","title":"Podman Run","text":"<pre><code># Create a volume for Glint's database\npodman volume create glint-data\n\n# Start Glint\npodman run -d \\\n  --name glint \\\n  --restart unless-stopped \\\n  -p 3800:3800 \\\n  -v glint-data:/data \\\n  -v $(pwd)/glint.yml:/etc/glint/glint.yml:ro \\\n  ghcr.io/darshan-rambhia/glint:latest \\\n  glint --config /etc/glint/glint.yml\n</code></pre>"},{"location":"deployment/#podman-compose","title":"Podman Compose","text":"<p>Install <code>podman-compose</code> if you don't have it:</p> <pre><code>pip install podman-compose\n</code></pre> <p>Then use the same <code>docker-compose.yml</code> from the Docker section:</p> <pre><code>podman-compose up -d\n</code></pre>"},{"location":"deployment/#quadlet-podman-systemd","title":"Quadlet (Podman + systemd)","text":"<p>Quadlet lets Podman containers be managed by systemd, so they start automatically on boot and can be controlled with <code>systemctl</code> commands.</p> <p>Step 1: Create the Quadlet file at <code>/etc/containers/systemd/glint.container</code>:</p> <pre><code>[Unit]\nDescription=Glint Proxmox Monitor\nAfter=network-online.target\nWants=network-online.target\n\n[Container]\nImage=ghcr.io/darshan-rambhia/glint:latest\nContainerName=glint\nPublishPort=3800:3800\nVolume=glint-data:/data\nVolume=/etc/glint/glint.yml:/etc/glint/glint.yml:ro\nExec=glint --config /etc/glint/glint.yml\n\n[Service]\nRestart=always\nTimeoutStartSec=120\n\n[Install]\nWantedBy=multi-user.target default.target\n</code></pre> <p>Step 2: Copy your config file into place:</p> <pre><code>sudo mkdir -p /etc/glint\nsudo cp glint.yml /etc/glint/glint.yml\n</code></pre> <p>Step 3: Start the service:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl start glint\nsudo systemctl enable glint\n</code></pre> <p>Step 4: Check that it's running:</p> <pre><code>sudo systemctl status glint\n</code></pre> <p>To update: <code>podman pull ghcr.io/darshan-rambhia/glint:latest &amp;&amp; sudo systemctl restart glint</code></p>"},{"location":"deployment/#homebrew","title":"Homebrew","text":"<p>Install Glint with Homebrew on macOS or Linux:</p> <pre><code>brew install darshan-rambhia/tap/glint\n</code></pre> <p>This installs the <code>glint</code> binary and the SQLite dependency automatically.</p> <p>Run Glint:</p> <pre><code>glint --config /path/to/glint.yml\n</code></pre> <p>Update to the latest version:</p> <pre><code>brew upgrade glint\n</code></pre> <p>Uninstall:</p> <pre><code>brew uninstall glint\nbrew untap darshan-rambhia/tap\n</code></pre>"},{"location":"deployment/#go-install","title":"Go Install","text":"<p>If you have Go 1.26+ installed, you can install Glint directly from source:</p> <pre><code>CGO_ENABLED=1 go install github.com/darshan-rambhia/glint/cmd/glint@latest\n</code></pre> <p>CGO is required</p> <p>Glint uses SQLite, which requires a C compiler. <code>CGO_ENABLED=1</code> tells Go to use it. On most Linux systems, <code>gcc</code> is already installed. On Debian/Ubuntu, install it with <code>sudo apt install build-essential</code>. On macOS, install Xcode command line tools with <code>xcode-select --install</code>.</p> <p>The binary is installed to <code>$GOPATH/bin</code> (usually <code>~/go/bin</code>). Make sure this is in your <code>PATH</code>:</p> <pre><code># Check if it's already in your PATH\nglint --version\n\n# If not found, add it (add this line to your ~/.bashrc or ~/.zshrc to make it permanent)\nexport PATH=$PATH:$(go env GOPATH)/bin\n</code></pre> <p>To install a specific version:</p> <pre><code>CGO_ENABLED=1 go install github.com/darshan-rambhia/glint/cmd/glint@v0.1.0\n</code></pre> <p>Once installed, create a config file and run it:</p> <pre><code>glint --config glint.yml\n</code></pre> <p>To run it as a system service, continue to Step 3: Create a System User below (skip the download step since you already have the binary --- just copy it to <code>/usr/local/bin</code>):</p> <pre><code>sudo cp $(go env GOPATH)/bin/glint /usr/local/bin/glint\n</code></pre>"},{"location":"deployment/#binary-install-no-containers","title":"Binary Install (No Containers)","text":"<p>This section is for running Glint directly on a Linux machine without Docker or Podman. Glint is a single file --- you download it, create a config file, and run it.</p>"},{"location":"deployment/#what-you-need","title":"What You Need","text":"<ul> <li>A Linux machine (Debian, Ubuntu, Proxmox, etc.)</li> <li><code>curl</code> installed (usually pre-installed)</li> <li><code>sudo</code> access (administrator privileges)</li> <li>Your Proxmox API tokens (see Getting Started)</li> </ul>"},{"location":"deployment/#step-1-check-your-architecture","title":"Step 1: Check Your Architecture","text":"<p>Glint is available for two CPU types. Run this command to find out which one your machine uses:</p> <pre><code>uname -m\n</code></pre> <ul> <li>If it says <code>x86_64</code> --- you have a standard 64-bit Intel/AMD processor (most common)</li> <li>If it says <code>aarch64</code> --- you have an ARM processor (Raspberry Pi 4/5, Oracle Cloud free tier, etc.)</li> </ul>"},{"location":"deployment/#step-2-download-glint","title":"Step 2: Download Glint","text":"x86_64 (Intel/AMD)aarch64 (ARM)Auto-detect <pre><code># Download the latest release\nVERSION=$(curl -s https://api.github.com/repos/darshan-rambhia/glint/releases/latest | grep tag_name | cut -d'\"' -f4)\ncurl -sL \"https://github.com/darshan-rambhia/glint/releases/download/${VERSION}/glint_${VERSION#v}_linux_amd64.tar.gz\" -o /tmp/glint.tar.gz\n\n# Extract the binary to /usr/local/bin (where Linux looks for programs)\nsudo tar -xzf /tmp/glint.tar.gz -C /usr/local/bin glint\n\n# Clean up the download\nrm /tmp/glint.tar.gz\n</code></pre> <pre><code># Download the latest release\nVERSION=$(curl -s https://api.github.com/repos/darshan-rambhia/glint/releases/latest | grep tag_name | cut -d'\"' -f4)\ncurl -sL \"https://github.com/darshan-rambhia/glint/releases/download/${VERSION}/glint_${VERSION#v}_linux_arm64.tar.gz\" -o /tmp/glint.tar.gz\n\n# Extract the binary to /usr/local/bin (where Linux looks for programs)\nsudo tar -xzf /tmp/glint.tar.gz -C /usr/local/bin glint\n\n# Clean up the download\nrm /tmp/glint.tar.gz\n</code></pre> <p>This script detects your architecture automatically:</p> <pre><code>VERSION=$(curl -s https://api.github.com/repos/darshan-rambhia/glint/releases/latest | grep tag_name | cut -d'\"' -f4)\nARCH=$(uname -m)\ncase $ARCH in\n  x86_64)  ARCH=amd64 ;;\n  aarch64) ARCH=arm64 ;;\nesac\n\ncurl -sL \"https://github.com/darshan-rambhia/glint/releases/download/${VERSION}/glint_${VERSION#v}_linux_${ARCH}.tar.gz\" -o /tmp/glint.tar.gz\nsudo tar -xzf /tmp/glint.tar.gz -C /usr/local/bin glint\nrm /tmp/glint.tar.gz\n</code></pre> <p>Verify the download worked:</p> <pre><code>glint --version\n</code></pre> <p>You should see output like:</p> <pre><code>glint v0.1.0\n  commit:    abc1234def5678 (clean)\n  built:     2026-02-16T12:00:00Z\n  go:        go1.26\n  platform:  linux/amd64\n</code></pre> <p>Command not found?</p> <p>If you see <code>glint: command not found</code>, the binary might not be in your PATH. Try running it with the full path: <code>/usr/local/bin/glint --version</code>. If that works, add <code>/usr/local/bin</code> to your PATH.</p>"},{"location":"deployment/#step-3-create-a-system-user","title":"Step 3: Create a System User","text":"<p>For security, Glint should run as its own user instead of as root. This limits what it can access on your system.</p> <pre><code>sudo useradd -r -s /usr/sbin/nologin -d /var/lib/glint glint\n</code></pre> <p>What does this command do?</p> <ul> <li><code>-r</code> --- creates a \"system\" user (no home directory login, lower UID)</li> <li><code>-s /usr/sbin/nologin</code> --- prevents anyone from logging in as this user</li> <li><code>-d /var/lib/glint</code> --- sets the home directory (where Glint stores its database)</li> <li><code>glint</code> --- the username</li> </ul>"},{"location":"deployment/#step-4-create-directories","title":"Step 4: Create Directories","text":"<p>Glint needs two directories:</p> <ul> <li><code>/var/lib/glint</code> --- where the SQLite database lives (read/write)</li> <li><code>/etc/glint</code> --- where the config file lives (read-only)</li> </ul> <pre><code># Create the data directory and give the glint user ownership\nsudo mkdir -p /var/lib/glint\nsudo chown glint:glint /var/lib/glint\n\n# Create the config directory\nsudo mkdir -p /etc/glint\n</code></pre>"},{"location":"deployment/#step-5-create-the-config-file","title":"Step 5: Create the Config File","text":"<p>Create the config file at <code>/etc/glint/glint.yml</code>. You can use <code>nano</code> (a simple text editor) or any editor you prefer:</p> <pre><code>sudo nano /etc/glint/glint.yml\n</code></pre> <p>Paste the following content, replacing the placeholder values with your actual Proxmox details:</p> <pre><code># Where to store the database (must match the directory from Step 4)\ndb_path: \"/var/lib/glint/glint.db\"\n\n# Web server settings\nlisten: \":3800\"\n\n# Logging (json is recommended for systemd, see the Logging page for details)\nlog_level: \"info\"\nlog_format: \"json\"\n\n# Your Proxmox VE server\npve:\n  - name: \"main\"\n    host: \"https://YOUR_PROXMOX_IP:8006\"       # &lt;-- Replace with your Proxmox IP\n    token_id: \"glint@pam!monitor\"               # &lt;-- Replace with your token ID\n    token_secret: \"YOUR_TOKEN_SECRET_HERE\"       # &lt;-- Replace with your token secret\n    insecure: true                               # Set to true if using self-signed certs\n\n# Your Proxmox Backup Server (remove this section if you don't use PBS)\npbs:\n  - name: \"main-pbs\"\n    host: \"https://YOUR_PBS_IP:8007\"            # &lt;-- Replace with your PBS IP\n    token_id: \"glint@pbs!monitor\"               # &lt;-- Replace with your PBS token ID\n    token_secret: \"YOUR_PBS_TOKEN_SECRET_HERE\"   # &lt;-- Replace with your PBS token secret\n    insecure: true\n    datastores: [\"homelab\"]                      # &lt;-- Replace with your datastore name(s)\n</code></pre> <p>Saving in nano</p> <p>Press <code>Ctrl+O</code> then <code>Enter</code> to save. Press <code>Ctrl+X</code> to exit.</p> <p>Don't have PBS?</p> <p>If you don't use Proxmox Backup Server, delete the entire <code>pbs:</code> section from the config file. Glint works fine with just PVE.</p> <p>Set secure permissions on the config file (it contains API tokens):</p> <pre><code>sudo chmod 640 /etc/glint/glint.yml\nsudo chown root:glint /etc/glint/glint.yml\n</code></pre> <p>What do these permissions mean?</p> <ul> <li><code>640</code> means: the owner (root) can read and write, the group (glint) can read, and nobody else can access it</li> <li><code>root:glint</code> means: owned by root, but the glint group can read it --- so the Glint service can read the config but not modify it</li> </ul>"},{"location":"deployment/#step-6-test-the-config","title":"Step 6: Test the Config","text":"<p>Before setting up the service, make sure Glint can start with your config:</p> <pre><code># Run Glint as the glint user to test (press Ctrl+C to stop)\nsudo -u glint /usr/local/bin/glint --config /etc/glint/glint.yml\n</code></pre> <p>You should see log output like:</p> <pre><code>{\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"starting glint\",\"version\":\"v0.1.0\",\"listen\":\":3800\"}\n{\"time\":\"...\",\"level\":\"INFO\",\"msg\":\"collector started\",\"name\":\"pve:main\",\"interval\":\"15s\"}\n</code></pre> <p>If you see errors like <code>401 Unauthorized</code> or <code>connection refused</code>, check the Troubleshooting section below.</p> <p>Press <code>Ctrl+C</code> to stop the test.</p>"},{"location":"deployment/#step-7-create-the-systemd-service","title":"Step 7: Create the Systemd Service","text":"<p>Systemd is the program that manages services on Linux. Creating a \"service file\" tells systemd how to run Glint, when to start it, and what to do if it crashes.</p> <p>Create the service file at <code>/etc/systemd/system/glint.service</code>:</p> <pre><code>sudo nano /etc/systemd/system/glint.service\n</code></pre> <p>Paste this content:</p> <pre><code>[Unit]\n# A human-readable description (shows up in \"systemctl status\")\nDescription=Glint Proxmox Monitor\nDocumentation=https://github.com/darshan-rambhia/glint\n\n# Wait for the network to be available before starting\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\n# \"simple\" means the process itself is the service (no forking)\nType=simple\n\n# Run as the dedicated glint user (not root)\nUser=glint\nGroup=glint\n\n# The command to start Glint\nExecStart=/usr/local/bin/glint --config /etc/glint/glint.yml\n\n# If Glint crashes, wait 5 seconds and restart it automatically\nRestart=on-failure\nRestartSec=5\n\n# Allow up to 65536 open files (generous limit for SQLite + HTTP connections)\nLimitNOFILE=65536\n\n# --- Security hardening ---\n# These settings restrict what the Glint process can do, reducing the impact\n# if the process is ever compromised.\n\n# Prevent the process from gaining new privileges\nNoNewPrivileges=yes\n\n# Make the entire filesystem read-only, except for specific paths below\nProtectSystem=strict\n\n# Hide /home, /root, and /run/user from the process\nProtectHome=yes\n\n# Allow writing ONLY to the data directory (for the SQLite database)\nReadWritePaths=/var/lib/glint\n\n# Allow reading the config file\nReadOnlyPaths=/etc/glint\n\n# Give the process its own /tmp (can't see other processes' temp files)\nPrivateTmp=yes\n\n# Hide physical devices (/dev) from the process\nPrivateDevices=yes\n\n# Prevent modifying kernel settings\nProtectKernelTunables=yes\nProtectKernelModules=yes\nProtectControlGroups=yes\n\n[Install]\n# Start this service when the system reaches \"multi-user\" mode (normal boot)\nWantedBy=multi-user.target\n</code></pre> <p>Save and exit (<code>Ctrl+O</code>, <code>Enter</code>, <code>Ctrl+X</code> in nano).</p>"},{"location":"deployment/#step-8-start-the-service","title":"Step 8: Start the Service","text":"<p>Tell systemd to load the new service file, enable it to start on boot, and start it now:</p> <pre><code># Reload systemd so it sees the new file\nsudo systemctl daemon-reload\n\n# Enable = start automatically on every boot\nsudo systemctl enable glint\n\n# Start it right now\nsudo systemctl start glint\n</code></pre>"},{"location":"deployment/#step-9-verify-its-running","title":"Step 9: Verify It's Running","text":"<pre><code>sudo systemctl status glint\n</code></pre> <p>You should see output like:</p> <pre><code>\u25cf glint.service - Glint Proxmox Monitor\n     Loaded: loaded (/etc/systemd/system/glint.service; enabled; preset: enabled)\n     Active: active (running) since Sun 2026-02-16 12:00:00 UTC; 5s ago\n       Docs: https://github.com/darshan-rambhia/glint\n   Main PID: 12345 (glint)\n      Tasks: 8 (limit: 4915)\n     Memory: 34.2M\n        CPU: 120ms\n     CGroup: /system.slice/glint.service\n             \u2514\u250012345 /usr/local/bin/glint --config /etc/glint/glint.yml\n</code></pre> <p>What to look for</p> <ul> <li><code>Active: active (running)</code> --- Glint is running</li> <li><code>enabled</code> --- will start automatically on boot</li> </ul> <p>Check the health endpoint:</p> <pre><code>curl http://localhost:3800/healthz\n</code></pre> <p>Open your browser and go to <code>http://YOUR_SERVER_IP:3800</code> to see the dashboard.</p>"},{"location":"deployment/#viewing-logs","title":"Viewing Logs","text":"<p>Logs are stored in the systemd journal. Here are the most useful commands:</p> <pre><code># Follow logs in real-time (press Ctrl+C to stop)\nsudo journalctl -u glint -f\n\n# Show the last 50 log lines\nsudo journalctl -u glint -n 50\n\n# Show logs from the last hour\nsudo journalctl -u glint --since \"1 hour ago\"\n\n# Show logs since the last system boot\nsudo journalctl -u glint -b\n\n# Show only errors\nsudo journalctl -u glint --priority=err\n</code></pre>"},{"location":"deployment/#stopping-and-restarting","title":"Stopping and Restarting","text":"<pre><code># Stop Glint\nsudo systemctl stop glint\n\n# Start Glint\nsudo systemctl start glint\n\n# Restart Glint (stop + start)\nsudo systemctl restart glint\n\n# Disable auto-start on boot\nsudo systemctl disable glint\n</code></pre>"},{"location":"deployment/#updating-to-a-new-version","title":"Updating to a New Version","text":"<p>When a new version of Glint is released:</p> <pre><code># 1. Download the new version (replace with actual version number)\nVERSION=v0.2.0\nARCH=amd64  # or arm64 for ARM\n\ncurl -sL \"https://github.com/darshan-rambhia/glint/releases/download/${VERSION}/glint_${VERSION#v}_linux_${ARCH}.tar.gz\" -o /tmp/glint.tar.gz\nsudo tar -xzf /tmp/glint.tar.gz -C /usr/local/bin glint\nrm /tmp/glint.tar.gz\n\n# 2. Restart the service to use the new version\nsudo systemctl restart glint\n\n# 3. Verify the new version is running\nglint --version\nsudo systemctl status glint\n</code></pre> <p>Auto-detect version script</p> <p>To always download the latest version automatically:</p> <pre><code>VERSION=$(curl -s https://api.github.com/repos/darshan-rambhia/glint/releases/latest | grep tag_name | cut -d'\"' -f4)\nARCH=$(uname -m)\ncase $ARCH in\n  x86_64)  ARCH=amd64 ;;\n  aarch64) ARCH=arm64 ;;\nesac\n\ncurl -sL \"https://github.com/darshan-rambhia/glint/releases/download/${VERSION}/glint_${VERSION#v}_linux_${ARCH}.tar.gz\" -o /tmp/glint.tar.gz\nsudo tar -xzf /tmp/glint.tar.gz -C /usr/local/bin glint\nrm /tmp/glint.tar.gz\nsudo systemctl restart glint\necho \"Updated to $(glint --version | head -1)\"\n</code></pre>"},{"location":"deployment/#uninstalling","title":"Uninstalling","text":"<p>To completely remove Glint from your system:</p> <pre><code># 1. Stop and disable the service\nsudo systemctl stop glint\nsudo systemctl disable glint\n\n# 2. Remove the service file\nsudo rm /etc/systemd/system/glint.service\nsudo systemctl daemon-reload\n\n# 3. Remove the binary\nsudo rm /usr/local/bin/glint\n\n# 4. Remove config and data (WARNING: deletes your database and history)\nsudo rm -rf /etc/glint\nsudo rm -rf /var/lib/glint\n\n# 5. Remove the system user\nsudo userdel glint\n</code></pre>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#common-issues","title":"Common Issues","text":"Problem What it looks like Solution Wrong API token <code>401 Unauthorized</code> in logs Double-check <code>token_id</code> and <code>token_secret</code> in your config. Format is <code>user@realm!tokenname</code>. Token lacks permissions <code>403 Forbidden</code> in logs Re-run: <code>pveum aclmod / -user glint@pam -role PVEAuditor</code> Can't reach Proxmox <code>connection refused</code> in logs Check that the IP and port are correct, and that Glint's machine can reach the Proxmox server. Try: <code>curl -k https://YOUR_PVE_IP:8006/api2/json/version</code> TLS/SSL error <code>TLS handshake error</code> in logs Set <code>insecure: true</code> in your config if using self-signed certificates. Service won't start <code>Active: failed</code> in <code>systemctl status</code> Check logs: <code>sudo journalctl -u glint -n 50</code>. Common causes: invalid YAML syntax, wrong file permissions, missing config file. Permission denied <code>permission denied</code> for database file Make sure <code>/var/lib/glint</code> is owned by the glint user: <code>sudo chown glint:glint /var/lib/glint</code> Port already in use <code>bind: address already in use</code> Something else is using port 3800. Either stop that service or change <code>listen</code> in your config to a different port, e.g., <code>listen: \":3801\"</code>. No data on dashboard Dashboard loads but shows no metrics Wait 15 seconds for the first poll. If still empty, check logs for collector errors. No disk data Dashboard shows nodes and guests but no disks S.M.A.R.T. data is polled once per hour. Wait up to 1 hour after first start, or set <code>disk_poll_interval: \"1m\"</code> temporarily for testing."},{"location":"deployment/#testing-your-api-token","title":"Testing Your API Token","text":"<p>You can test your API token directly from the command line to verify it works:</p> <pre><code># Test PVE token (replace with your values)\ncurl -k -H \"Authorization: PVEAPIToken=glint@pam!monitor=YOUR_SECRET\" \\\n  https://YOUR_PVE_IP:8006/api2/json/version\n</code></pre> <p>If the token is valid, you'll see a JSON response with version info. If not, you'll see a <code>401</code> error.</p> <pre><code># Test PBS token (replace with your values)\ncurl -k -H \"Authorization: PBSAPIToken=glint@pbs!monitor:YOUR_SECRET\" \\\n  https://YOUR_PBS_IP:8007/api2/json/status/datastore-usage\n</code></pre> <p>PVE uses <code>=</code> between token name and secret, PBS uses <code>:</code></p> <ul> <li>PVE: <code>PVEAPIToken=user@realm!token=secret</code></li> <li>PBS: <code>PBSAPIToken=user@realm!token:secret</code></li> </ul>"},{"location":"deployment/#checking-if-glint-can-reach-proxmox","title":"Checking if Glint Can Reach Proxmox","text":"<p>From the machine where Glint is running:</p> <pre><code># Test network connectivity to PVE (should show \"Connected\")\ncurl -k -s -o /dev/null -w \"HTTP %{http_code}\\n\" https://YOUR_PVE_IP:8006/api2/json/version\n\n# Test network connectivity to PBS (if using)\ncurl -k -s -o /dev/null -w \"HTTP %{http_code}\\n\" https://YOUR_PBS_IP:8007/api2/json/version\n</code></pre> <ul> <li><code>HTTP 200</code> or <code>HTTP 401</code> --- network is fine (401 just means you didn't pass a token)</li> <li><code>HTTP 000</code> or <code>Connection refused</code> --- Glint can't reach the server (check firewall, IP, port)</li> </ul>"},{"location":"deployment/#resource-usage","title":"Resource Usage","text":"<p>Glint is designed to be lightweight:</p> Resource Typical Limit RAM 30-50 MB 128 MB CPU Negligible --- Disk (binary) ~10 MB --- Disk (data) ~20-50 MB Depends on history retention"},{"location":"getting-started/","title":"Getting Started","text":"<p>This guide walks you through setting up Glint end-to-end: creating Proxmox API tokens, writing your config file, and deploying with Docker Compose.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Glint connects to Proxmox APIs using API tokens (not passwords). Tokens are scoped to specific permissions and can be revoked independently. Glint only needs read-only access.</p> Integration Role Required Permissions Proxmox VE <code>PVEAuditor</code> Read-only access to nodes, VMs, containers, disks, and SMART data Proxmox Backup Server <code>Audit</code> Read-only access to datastores, snapshots, and tasks"},{"location":"getting-started/#1-create-proxmox-api-tokens","title":"1. Create Proxmox API Tokens","text":""},{"location":"getting-started/#proxmox-ve-pve","title":"Proxmox VE (PVE)","text":"<p>SSH into your Proxmox VE host:</p> <pre><code># Create a dedicated user in the PAM realm\npveum user add glint@pam --comment \"Glint monitoring\"\n\n# Assign the built-in PVEAuditor role (read-only on all resources)\npveum aclmod / -user glint@pam -role PVEAuditor\n\n# Create an API token (--privsep 0 inherits the user's role)\npveum user token add glint@pam monitor --privsep 0\n</code></pre> <p>Save the token secret</p> <p>The token secret is only shown once. Copy it immediately.</p> <p>PVEAuditor grants read-only access to: node status and metrics, LXC/QEMU lists, disk list and SMART data, cluster status, node discovery. It cannot start/stop VMs, change configuration, or access the console.</p> <p>Verify with:</p> <pre><code>curl -k -H \"Authorization: PVEAPIToken=glint@pam!monitor=YOUR_SECRET\" \\\n  https://YOUR_PVE_IP:8006/api2/json/nodes\n</code></pre>"},{"location":"getting-started/#proxmox-backup-server-pbs","title":"Proxmox Backup Server (PBS)","text":"<p>SSH into your PBS host:</p> <pre><code># Create a dedicated user\nproxmox-backup-manager user create glint@pbs --comment \"Glint monitoring\"\n\n# Assign the Audit role (read-only)\nproxmox-backup-manager acl update / Audit --auth-id glint@pbs\n\n# Create an API token\nproxmox-backup-manager user generate-token glint@pbs monitor\n</code></pre> <p>Verify with:</p> <pre><code>curl -k -H \"Authorization: PBSAPIToken=glint@pbs!monitor:YOUR_SECRET\" \\\n  https://YOUR_PBS_IP:8007/api2/json/status/datastore-usage\n</code></pre>"},{"location":"getting-started/#2-write-your-config-file","title":"2. Write Your Config File","text":"<p>Create <code>glint.yml</code> from the example:</p> <pre><code>cp glint.example.yml glint.yml\n</code></pre> <p>Here is a complete annotated config:</p> <pre><code># Server\nlisten: \":3800\"                # Address to bind the HTTP server\ndb_path: \"/data/glint.db\"      # SQLite database path (must be writable)\nlog_level: \"info\"              # debug, info, warn, error\nlog_format: \"text\"             # \"text\" for human-readable, \"json\" for structured\nhistory_hours: 48              # How many hours of sparkline history to keep\nworker_pool_size: 4            # Max concurrent API calls across all collectors\n\n# Proxmox VE instances (at least one required)\npve:\n  - name: \"main\"                                       # Unique label\n    host: \"https://192.168.1.215:8006\"                  # PVE API URL\n    token_id: \"glint@pam!monitor\"                       # user@realm!tokenname\n    token_secret: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\" # Token secret\n    insecure: true                                      # Allow self-signed TLS certs\n    poll_interval: \"15s\"                                # How often to poll metrics\n    disk_poll_interval: \"1h\"                            # How often to poll SMART data\n\n# Proxmox Backup Server instances (optional)\npbs:\n  - name: \"main-pbs\"\n    host: \"https://10.100.1.102:8007\"\n    token_id: \"glint@pbs!monitor\"\n    token_secret: \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\"\n    insecure: true\n    datastores: [\"homelab\"]     # Which datastores to monitor (empty = all)\n    poll_interval: \"5m\"\n\n# Notification targets (optional)\nnotifications:\n  - type: ntfy\n    url: \"http://10.100.1.104:8080\"\n    topic: \"homelab-alerts\"\n\n# Alert thresholds (optional --- defaults are sensible)\nalerts:\n  node_cpu_high:\n    threshold: 90\n    duration: \"5m\"\n    severity: \"warning\"\n  guest_down:\n    grace_period: \"2m\"\n    severity: \"critical\"\n  backup_stale:\n    max_age: \"36h\"\n    severity: \"warning\"\n  disk_smart_failed:\n    severity: \"critical\"\n  datastore_full:\n    threshold: 85\n    severity: \"warning\"\n</code></pre> <p>Environment variables</p> <p>For a single PVE + PBS instance, you can skip the config file entirely and use environment variables. See the Configuration page for details.</p>"},{"location":"getting-started/#3-deploy-with-docker-compose","title":"3. Deploy with Docker Compose","text":"<p>Create a <code>docker-compose.yml</code> alongside your <code>glint.yml</code>:</p> <pre><code>services:\n  glint:\n    image: ghcr.io/darshan-rambhia/glint:latest\n    container_name: glint\n    restart: unless-stopped\n    ports:\n      - \"3800:3800\"\n    volumes:\n      - glint-data:/data\n      - ./glint.yml:/etc/glint/glint.yml:ro\n    command: [\"glint\", \"--config\", \"/etc/glint/glint.yml\"]\n    deploy:\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 32M\n    logging:\n      driver: json-file\n      options:\n        max-size: \"10m\"\n        max-file: \"3\"\n\nvolumes:\n  glint-data:\n</code></pre> <p>Scratch base image</p> <p>The Glint image uses a <code>scratch</code> base (no shell, no utilities), so in-container <code>HEALTHCHECK</code> is not available. Use <code>curl http://localhost:3800/healthz</code> from the host or configure health checks in your reverse proxy.</p> <p>Start it:</p> <pre><code>docker compose up -d\n\n# Check logs\ndocker compose logs -f glint\n\n# Verify health\ncurl http://localhost:3800/healthz\n</code></pre>"},{"location":"getting-started/#4-verify-the-installation","title":"4. Verify the Installation","text":"<p>After starting Glint:</p> <ol> <li>Health check: <code>curl http://localhost:3800/healthz</code></li> <li>Dashboard: Open <code>http://localhost:3800</code> in your browser</li> <li>Logs: Check for collector startup messages --- you should see <code>PVE collection complete</code> within 15 seconds</li> </ol>"},{"location":"getting-started/#5-troubleshooting","title":"5. Troubleshooting","text":"Issue Solution <code>401 Unauthorized</code> Token ID or secret is wrong. Format: <code>user@realm!tokenname</code> <code>403 Forbidden</code> Token lacks permissions. Re-run <code>pveum aclmod / -user glint@pam -role PVEAuditor</code> <code>Connection refused</code> PVE/PBS host unreachable. Check network, firewall, and port (8006/8007) <code>TLS handshake error</code> Set <code>insecure: true</code> for self-signed certificates <code>No nodes found</code> Token may not have permissions on <code>/nodes</code>. Re-assign PVEAuditor on <code>/</code> <code>No disks found</code> SMART data is polled hourly. Wait up to 1 hour for initial disk data <code>No data on dashboard</code> Check logs for collector errors. Verify API tokens with <code>curl</code> (see step 1) Container won't start Check <code>docker compose logs glint</code>. Common: invalid YAML, missing required fields"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration --- full reference for all YAML options and environment variables</li> <li>Deployment --- Podman, Quadlet, and systemd bare metal options</li> <li>Logging --- structured logging for systemd/journald</li> </ul>"},{"location":"logging/","title":"Logging","text":"<p>Glint uses Go's structured <code>slog</code> library. All log output goes to stderr.</p>"},{"location":"logging/#log-format","title":"Log Format","text":"<p>Glint supports two output formats, configured via <code>log_format</code> in the config file or the <code>GLINT_LOG_FORMAT</code> environment variable.</p> Text (default)JSON <p>Human-readable key=value format. Best for local development and interactive debugging.</p> <pre><code>time=2026-02-16T12:00:00.000Z level=INFO msg=\"starting glint\" version=0.1.0 commit=abc1234 listen=:3800\ntime=2026-02-16T12:00:00.015Z level=INFO msg=\"collector started\" name=pve:main interval=15s\ntime=2026-02-16T12:00:15.032Z level=DEBUG msg=\"PVE collection complete\" instance=main nodes=1 guests=8\n</code></pre> <p>Structured JSON, one object per line. Best for systemd/journald, Docker json-file driver, and log aggregators.</p> <pre><code>{\"time\":\"2026-02-16T12:00:00.000Z\",\"level\":\"INFO\",\"msg\":\"starting glint\",\"version\":\"0.1.0\",\"commit\":\"abc1234\",\"listen\":\":3800\"}\n{\"time\":\"2026-02-16T12:00:00.015Z\",\"level\":\"INFO\",\"msg\":\"collector started\",\"name\":\"pve:main\",\"interval\":\"15s\"}\n</code></pre> <p>Set via config:</p> <pre><code>log_format: \"json\"\n</code></pre> <p>Or environment variable:</p> <pre><code>export GLINT_LOG_FORMAT=json\n</code></pre>"},{"location":"logging/#log-levels","title":"Log Levels","text":"Level What it logs <code>error</code> Failures that affect functionality (DB errors, API failures) <code>warn</code> Degraded behavior (alerts fired, cluster detection fallback) <code>info</code> Lifecycle events (startup, shutdown, prune results) <code>debug</code> Per-poll collection details, HTTP request logs <p>Set via config (<code>log_level: \"debug\"</code>) or env var (<code>GLINT_LOG_LEVEL=debug</code>).</p> <p>Production recommendation</p> <p>Use <code>log_level: \"info\"</code> and <code>log_format: \"json\"</code> for production. This gives you structured logs that are easy to filter while keeping volume manageable.</p>"},{"location":"logging/#docker-logging","title":"Docker Logging","text":"<p>Docker captures container stdout/stderr automatically. The <code>json-file</code> logging driver (default) stores logs on disk. The recommended <code>docker-compose.yml</code> includes rotation settings:</p> <pre><code>logging:\n  driver: json-file\n  options:\n    max-size: \"10m\"\n    max-file: \"3\"\n</code></pre>"},{"location":"logging/#viewing-logs","title":"Viewing Logs","text":"<pre><code># Follow logs\ndocker compose logs -f glint\n\n# Filter by level (requires json log format)\ndocker compose logs glint | jq 'select(.level == \"ERROR\")'\n\n# Show last 100 lines\ndocker compose logs --tail=100 glint\n</code></pre>"},{"location":"logging/#systemd-journald","title":"systemd / journald","text":"<p>When running as a systemd service (bare metal or Podman Quadlet), set <code>log_format: \"json\"</code> for best integration. systemd captures stderr and stores it in the journal.</p>"},{"location":"logging/#viewing-logs_1","title":"Viewing Logs","text":"<pre><code># Follow logs\njournalctl -u glint -f\n\n# View logs since last boot\njournalctl -u glint -b\n\n# Filter by priority (requires json format for structured fields)\njournalctl -u glint --priority=err\n\n# Export as JSON\njournalctl -u glint -o json\n\n# Show logs from last hour\njournalctl -u glint --since=\"1 hour ago\"\n</code></pre>"},{"location":"logging/#recommended-systemd-config","title":"Recommended systemd Config","text":"<pre><code>log_level: \"info\"\nlog_format: \"json\"\n</code></pre> <p>This gives you structured logs that journald can index and filter, while keeping the volume manageable.</p>"},{"location":"logging/#log-fields","title":"Log Fields","text":"<p>All log entries include these base fields:</p> Field Description Example <code>time</code> ISO 8601 timestamp <code>2026-02-16T12:00:00.000Z</code> <code>level</code> Log level <code>INFO</code>, <code>ERROR</code> <code>msg</code> Human-readable message <code>\"PVE collection complete\"</code> <p>Collector logs include additional context:</p> Field Description Example <code>instance</code> PVE/PBS instance name <code>\"main\"</code> <code>name</code> Collector identifier <code>\"pve:main\"</code> <code>interval</code> Poll interval <code>\"15s\"</code> <code>nodes</code> Number of nodes polled <code>1</code> <code>guests</code> Number of guests found <code>8</code> <code>duration</code> Collection duration <code>\"1.234s\"</code> <p>Alert logs include:</p> Field Description Example <code>alert_type</code> Rule that fired <code>\"node_cpu_high\"</code> <code>severity</code> Alert severity <code>\"warning\"</code> <code>subject</code> What triggered it <code>\"main/pve\"</code>"},{"location":"testing/","title":"Testing","text":"<p>Glint uses Go's built-in testing framework with <code>testify/assert</code> for assertions. The test suite includes unit tests, benchmark tests, fuzz tests, and integration tests.</p>"},{"location":"testing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\ntask test\n\n# Run tests with race detector\ntask test:race\n\n# Run tests with coverage tracking (enforces minimum threshold)\ntask test:coverage\n</code></pre>"},{"location":"testing/#coverage","title":"Coverage","text":"<p>Coverage is tracked via <code>buildscripts/coverage/main.go</code> with an auto-ratcheting threshold stored in <code>buildscripts/coverage/coverage_required.txt</code>.</p> <pre><code># Run coverage check\ntask test:coverage\n</code></pre> <p>The coverage tool will:</p> <ol> <li>Run all tests with <code>-race</code> and <code>-coverprofile</code></li> <li>Print per-function coverage</li> <li>Auto-ratchet the threshold upward if coverage improved</li> <li>Fail the build if coverage dropped below threshold</li> <li>Generate an HTML report at <code>target/reports/coverage.html</code></li> </ol> <p>Ratcheting threshold</p> <p>The coverage threshold only goes up, never down. If a PR drops coverage below the threshold, CI will fail.</p> <p>View the HTML coverage report:</p> <pre><code>open target/reports/coverage.html\n</code></pre>"},{"location":"testing/#benchmark-tests","title":"Benchmark Tests","text":"<p>Benchmark tests measure the performance of hot paths. They live alongside unit tests in <code>*_test.go</code> files.</p>"},{"location":"testing/#running-benchmarks","title":"Running Benchmarks","text":"<pre><code># Run all benchmarks\ngo test ./... -bench=. -benchmem -count=3 -run=^$\n\n# Run benchmarks for a specific package\ngo test ./internal/smart/... -bench=. -benchmem -count=3 -run=^$\n\n# Run a specific benchmark\ngo test ./internal/smart/... -bench=BenchmarkEvaluateDisk -benchmem -count=5 -run=^$\n\n# Compare before/after\ngo test ./... -bench=. -benchmem -count=5 -run=^$ &gt; bench-before.txt\n# ... make changes ...\ngo test ./... -bench=. -benchmem -count=5 -run=^$ &gt; bench-after.txt\ngo install golang.org/x/perf/cmd/benchstat@latest\nbenchstat bench-before.txt bench-after.txt\n</code></pre>"},{"location":"testing/#current-benchmarks","title":"Current Benchmarks","text":"Package Benchmark Description <code>internal/smart</code> <code>BenchmarkEvaluateDisk</code> Evaluates all SMART attributes for a 12-attribute disk (~228ns/op)"},{"location":"testing/#writing-benchmarks","title":"Writing Benchmarks","text":"<pre><code>func BenchmarkMyFunction(b *testing.B) {\n    input := prepareInput()  // setup (not timed)\n    b.ResetTimer()\n    for i := 0; i &lt; b.N; i++ {\n        MyFunction(input)\n    }\n}\n</code></pre> <p>Good candidates for benchmarks:</p> <ul> <li>SMART threshold evaluation (called per-disk, per-attribute)</li> <li>Cache snapshot creation (called on every HTTP request)</li> <li>SQLite insert/query cycles (called every poll interval)</li> <li>Template rendering (called on every page load)</li> </ul>"},{"location":"testing/#fuzz-tests","title":"Fuzz Tests","text":"<p>Fuzz tests exercise parsers with random inputs to find panics, crashes, and unexpected behavior. They target code that handles untrusted external data (PVE/PBS API responses).</p>"},{"location":"testing/#running-fuzz-tests","title":"Running Fuzz Tests","text":"<pre><code># Run all fuzz tests for 30 seconds each\ngo test ./internal/smart/... -fuzz=. -fuzztime=30s\n\n# Run a specific fuzz test\ngo test ./internal/smart/... -fuzz=FuzzParseATARaw -fuzztime=60s\n\n# Longer duration for deeper coverage\ngo test ./internal/smart/... -fuzz=FuzzParseNVMeText -fuzztime=5m\n\n# Fuzz the collector response parsers\ngo test ./internal/collector/... -fuzz=FuzzParseNodeStatus -fuzztime=30s\n</code></pre>"},{"location":"testing/#current-fuzz-targets","title":"Current Fuzz Targets","text":"Package Fuzz Test Input Purpose <code>internal/smart</code> <code>FuzzParseATARaw</code> Random ATA <code>raw</code> strings Tests raw value extraction from strings like <code>\"40 (Min/Max 25/55)\"</code> <code>internal/smart</code> <code>FuzzParseNVMeText</code> Random smartctl text output Tests NVMe field extraction from free-form text <code>internal/collector</code> <code>FuzzParseNodeStatus</code> Random JSON Tests PVE node status response parsing <code>internal/collector</code> <code>FuzzParseLoadAvg</code> Random JSON arrays Tests loadavg parsing (strings vs floats) <code>internal/collector</code> <code>FuzzParseSensorsJSON</code> Random JSON Tests <code>sensors -j</code> output parsing"},{"location":"testing/#writing-fuzz-tests","title":"Writing Fuzz Tests","text":"<pre><code>func FuzzMyParser(f *testing.F) {\n    // Seed corpus with known-good inputs\n    f.Add([]byte(`{\"valid\": \"input\"}`))\n    f.Add([]byte(`{\"edge\": \"case\"}`))\n\n    f.Fuzz(func(t *testing.T, data []byte) {\n        result, err := MyParser(data)\n        if err != nil {\n            return // errors are fine, panics are not\n        }\n        // Validate invariants on successful parse\n        if result.Value &lt; 0 {\n            t.Errorf(\"negative value: %d\", result.Value)\n        }\n    })\n}\n</code></pre> <p>Fuzz corpus</p> <p>Fuzz corpus files are stored in <code>testdata/fuzz/</code> and committed to git. When a fuzz test finds a crash, the failing input is saved automatically and becomes a regression test.</p>"},{"location":"testing/#best-practices","title":"Best Practices","text":"<ul> <li>Seed with real data: Add actual PVE/PBS API responses as seed corpus entries</li> <li>Target parsers: Focus on functions that parse external/untrusted data</li> <li>Check invariants: Beyond \"no panic\", validate that output makes sense</li> <li>Run in CI: Fuzz tests run as regular tests (without <code>-fuzz</code> flag) using only the seed corpus, catching regressions</li> </ul>"},{"location":"testing/#integration-tests","title":"Integration Tests","text":"<p>Integration tests run against real Proxmox VE and PBS APIs. They are tagged with <code>//go:build integration</code> and skipped in normal test runs.</p> <pre><code>GLINT_PVE_URL=https://192.168.1.215:8006 \\\nGLINT_PVE_TOKEN_ID=glint@pam!monitor \\\nGLINT_PVE_TOKEN_SECRET=xxx \\\ngo test ./... -tags=integration -v -count=1\n</code></pre>"},{"location":"testing/#linting","title":"Linting","text":"<pre><code># Run all linters\ntask lint\n\n# Run with auto-fix\ngolangci-lint run --fix ./...\n</code></pre> <p>See <code>.golangci.yml</code> for the full linter configuration. Key linters enabled:</p> Linter Purpose <code>errcheck</code> Unchecked errors <code>gosec</code> Security issues <code>errorlint</code> Proper error wrapping <code>gocritic</code> Opinionated code quality <code>revive</code> Go style conventions <code>testifylint</code> testify best practices"},{"location":"testing/#markdown-linting","title":"Markdown Linting","text":"<p>Markdown files are linted with markdownlint:</p> <pre><code># Install\nnpm install -g markdownlint-cli2\n\n# Lint all markdown\nmarkdownlint-cli2 \"**/*.md\"\n\n# Fix auto-fixable issues\nmarkdownlint-cli2 --fix \"**/*.md\"\n</code></pre> <p>Configuration is in <code>.markdownlint.yaml</code>.</p>"}]}